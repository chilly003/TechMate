{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ex_tag(sid, date):\n",
    "\n",
    "#     url = f\"https://news.naver.com/breakingnews/section/105/{sid}\"\\\n",
    "#     f\"?date={date}\"\n",
    "#     html = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n",
    "#     \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "#     \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "#     soup = BeautifulSoup(html.text, \"lxml\")\n",
    "#     a_tag = soup.find_all(\"a\", class_=\"sa_text_title _NLOG_IMPRESSION\")\n",
    "    \n",
    "#     tag_lst = []\n",
    "#     print(len(a_tag))\n",
    "#     for a in a_tag:\n",
    "#         imp_url = a.get(\"data-imp-url\") \n",
    "#         if imp_url:  # hrefê°€ ìˆëŠ”ê²ƒë§Œ ê³ ë¥´ëŠ” ê²ƒ\n",
    "#             tag_lst.append(imp_url)  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "#             print(imp_url)  # í™•ì¸ìš© ì¶œë ¥\n",
    "                \n",
    "#     return tag_lst\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def ex_tag(sid, date):\n",
    "    # Selenium ì›¹ ë“œë¼ì´ë²„ ì„¤ì •\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    url = f\"https://news.naver.com/breakingnews/section/105/{sid}?date={date}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ìµœì´ˆ ë¡œë”© 2ì´ˆ ëŒ€ê¸°)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # \"ê¸°ì‚¬ ë”ë³´ê¸°\" ë²„íŠ¼ í´ë¦­ (í˜ì´ì§€ í•˜ë‹¨ê¹Œì§€ ìŠ¤í¬ë¡¤)\n",
    "    while True:\n",
    "        try:\n",
    "            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­\n",
    "            load_more_button = driver.find_element(By.CSS_SELECTOR, \"a.section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON\")\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # ë²„íŠ¼ í´ë¦­ í›„ ì ì‹œ ëŒ€ê¸°\n",
    "        except Exception as e:\n",
    "            # ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ìœ¼ë©´ (ì¦‰, ë” ì´ìƒ ë¡œë“œí•  ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´) ë°˜ë³µë¬¸ ì¢…ë£Œ\n",
    "            print(\"ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            break\n",
    "    \n",
    "    # í˜ì´ì§€ ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ BeautifulSoupìœ¼ë¡œ íŒŒì‹±\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    a_tag = soup.find_all(\"a\", class_=\"sa_text_title _NLOG_IMPRESSION\")\n",
    "    \n",
    "    tag_lst = []\n",
    "    print(len(a_tag))  # ì „ì²´ ê¸°ì‚¬ ê°œìˆ˜ í™•ì¸\n",
    "    for a in a_tag:\n",
    "        imp_url = a.get(\"data-imp-url\") \n",
    "        if imp_url:  # hrefê°€ ìˆëŠ” ê²ƒë§Œ ê³ ë¥´ëŠ” ê²ƒ\n",
    "            tag_lst.append(imp_url)  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "            # print(imp_url)  # í™•ì¸ìš© ì¶œë ¥\n",
    "    \n",
    "    # ë“œë¼ì´ë²„ ì¢…ë£Œ\n",
    "    driver.quit()\n",
    "    \n",
    "    return tag_lst\n",
    "\n",
    "# ì˜ˆì‹œ ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì¹´í…Œê³ ë¦¬ë³„ë¡œ html í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "64\n",
      "ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "105\n",
      "ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "122\n",
      "ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "709\n",
      "ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "21\n",
      "ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "66\n",
      "ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "category_id = [731, 226, 227, 230, 732, 283, 229] # ëª¨ë°”ì¼, ì¸í„°ë„·/SNS, í†µì‹ /ë‰´ë¯¸ë””ì–´, IT ì¼ë°˜, ë³´ì•ˆ/í•´í‚¹, ì»´í“¨í„°, ê²Œì„/ë¦¬ë·°\n",
    "category = [\"ëª¨ë°”ì¼\", \"SNS\", \"í†µì‹ \", \"IT ì¼ë°˜\", \"ë³´ì•ˆ\", \"AI\", \"ê²Œì„\"]\n",
    "today = \"20250225\"\n",
    "org_link = []\n",
    "\n",
    "for id in category_id:\n",
    "    org_link.append(ex_tag(id, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_tag(sid):\n",
    "    ### íŠ¹ì • ë¶„ì•¼ì˜ 100í˜ì´ì§€ê¹Œì§€ì˜ ë‰´ìŠ¤ì˜ ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì¤‘ë³µ ì œê±°í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ###\n",
    "    re_lst = []\n",
    "    for i in tqdm(range(100)):\n",
    "        lst = ex_tag(sid, i+1)\n",
    "        re_lst.extend(lst)\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    re_set = set(re_lst)\n",
    "    re_lst = list(re_set)\n",
    "    \n",
    "    return re_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì›ë¬¸ / í‚¤ì›Œë“œ ì¶”ì¶œ í›„ JSON í˜•ì‹ìœ¼ë¡œ DB Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def art_crawl(all_hrefs, category_idx, index):\n",
    "\n",
    "    url = all_hrefs[category_idx][index]\n",
    "    html = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 \"\\\n",
    "    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\"\\\n",
    "    \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html.text, \"lxml\")\n",
    "\n",
    "\n",
    "    # -------------------------------------- DATA ì¶”ì¶œ --------------------------------------\n",
    "\n",
    "    # ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find(\"meta\", property=\"og:title\")\n",
    "    news_title = \"\"\n",
    "    if title_tag is not None:\n",
    "        news_title = title_tag[\"content\"] if title_tag else soup.find(\"h2\", class_=\"media_end_head_headline\").text.strip()\n",
    "\n",
    "    # ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ\n",
    "    article_body = soup.find(\"article\", id=\"dic_area\")\n",
    "    news_content = \"\"\n",
    "    if article_body is not None:\n",
    "        news_content = article_body.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    summary = \"\"\n",
    "    if article_body:\n",
    "    # ìš”ì•½ ë¶€ë¶„ ì¶”ì¶œ (ìˆì„ ê²½ìš°)\n",
    "        summary_tag = article_body.find(\"strong\", class_=\"media_end_summary\")\n",
    "        summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "        # ë³¸ë¬¸ ë‚´ìš©ì—ì„œ ìš”ì•½ ë¶€ë¶„ ì œê±°\n",
    "        if summary_tag:\n",
    "            summary_tag.extract()  # summary_tagë¥¼ ì œê±°í•˜ì—¬ ë³¸ë¬¸ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ í•¨\n",
    "\n",
    "        # ë‚˜ë¨¸ì§€ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°)\n",
    "        news_content = article_body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        #print(\"ğŸ“Œ ìš”ì•½:\", summary)\n",
    "        #print(\"\\nğŸ“Œ ë³¸ë¬¸:\", content)\n",
    "\n",
    "    # ë‰´ìŠ¤ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    #image_tags = soup.find_all(\"meta\", property=\"og:image\")\n",
    "\n",
    "    # ì—¬ëŸ¬ ê°œì˜ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    image_data = []\n",
    "\n",
    "    # ëª¨ë“  end_photo_org íƒœê·¸ ì°¾ê¸°\n",
    "    photo_tags = soup.find_all(\"span\", class_=\"end_photo_org\")\n",
    "    #print(f\"photo tag ê°œìˆ˜: {len(photo_tags)}\")\n",
    "    for photo_tag in photo_tags:\n",
    "        # ì´ë¯¸ì§€ íƒœê·¸ ì°¾ê¸°\n",
    "        img_tag = photo_tag.find(\"img\")\n",
    "        #print(img_tag)\n",
    "        img_url = img_tag[\"data-src\"] if img_tag else None  # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„\n",
    "        #print(f\"url: {img_url}\")\n",
    "        # ì´ë¯¸ì§€ ì„¤ëª… íƒœê·¸ ì°¾ê¸° (ì—†ì„ ìˆ˜ë„ ìˆìŒ)\n",
    "        desc_tag = photo_tag.find(\"em\", class_=\"img_desc\")\n",
    "        img_desc = desc_tag.get_text(strip=True) if desc_tag else None  # ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš° None\n",
    "        #print(f\"ì„¤ëª…: {img_desc}\")\n",
    "        # ìœ íš¨í•œ ì´ë¯¸ì§€ ë°ì´í„°ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "        if img_url:\n",
    "            image_data.append({\"image_url\": img_url, \"image_desc\": img_desc})\n",
    "\n",
    "\n",
    "    news_images = []\n",
    "    # img_list.append()\n",
    "    # ë‰´ìŠ¤ ì‘ì„± ì‹œê°„ ì¶”ì¶œ\n",
    "    date_tag = soup.find('span', class_='media_end_head_info_datestamp_time _ARTICLE_DATE_TIME')\n",
    "    date_time = \"\"\n",
    "    if date_tag is not None:\n",
    "        date_time = date_tag['data-date-time']\n",
    "    # ê¸°ì ì´ë¦„ ì¶”ì¶œ\n",
    "    reporter_tag = soup.find('em', class_='media_end_head_journalist_name')\n",
    "    reporter_name = \"\"\n",
    "    if reporter_tag is not None:\n",
    "        reporter_name = reporter_tag.text.strip()\n",
    "        \n",
    "    # ì–¸ë¡ ì‚¬ ì´ë¦„ ì¶”ì¶œ\n",
    "    media_tag = soup.find(\"a\", class_=\"media_end_head_top_logo\")\n",
    "    media_name = media_tag.find(\"img\", class_=\"media_end_head_top_logo_img\")[\"alt\"] if media_tag else \"\"\n",
    "    # print(media_name)\n",
    "    journal = \"\"\n",
    "    # print(\"ğŸ“Œ ì œëª©:\", news_title)\n",
    "    # print(\"ğŸ“Œ ì–¸ë¡ ì‚¬:\", journal)\n",
    "    # print(\"ğŸ“Œ ê¸°ì:\", reporter_name)\n",
    "    # print(\"ğŸ“Œ ë‚ ì§œ:\", date_time)\n",
    "    # print(\"ğŸ“Œ ìš”ì•½:\", summary)\n",
    "    # print(\"\\nğŸ“Œ ë³¸ë¬¸:\", news_content)\n",
    "    # print(\"\\n ì´ë¯¸ì§€ ê°œìˆ˜: \", len(image_data))\n",
    "    if media_name is not None:\n",
    "        journal = media_name     \n",
    "    return news_title, news_content, image_data, date_time, reporter_name, journal, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import konlpy\n",
    "from krwordrank.word import summarize_with_keywords\n",
    "from konlpy.tag import Okt\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "def ext_keyword(cur_text):\n",
    "#     t = Okt()\n",
    "#     content_tokens = t.morphs(cur_text) # ê¸°ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "#     print(len(content_tokens))\n",
    "\n",
    "# í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ í¬í•¨í•´ì„œ ë½‘ì•„ë‚´ëŠ” ë°©ì‹\n",
    "\n",
    "    min_count = 1   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜ (ê·¸ë˜í”„ ìƒì„± ì‹œ)\n",
    "    max_length = 10 # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "    beta = 0.85    # PageRankì˜ decaying factor beta\n",
    "    max_iter = 20\n",
    "    #print(\"start\")\n",
    "    keywords, rank, graph = wordrank_extractor.extract([cur_text], beta, max_iter)\n",
    "    #print(len(keywords))\n",
    "\n",
    "    # í‚¤ì›Œë“œë¥¼ ì ìˆ˜ì— ë”°ë¼ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ê³  ìƒìœ„ 50ê°œë§Œ ì„ íƒ\n",
    "    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_keywords = sorted_keywords[:50]\n",
    "    \n",
    "    # ê° í‚¤ì›Œë“œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: {\"word\": \"í‚¤ì›Œë“œ\", \"score\": ì ìˆ˜})\n",
    "    keyword_list = [{\"word\": word, \"score\": r} for word, r in top_keywords]\n",
    "    \n",
    "    # JSON í˜•ì‹ì˜ ë°ì´í„° ìƒì„± (í•„ìš” ì‹œ ë‹¤ë¥¸ í•„ë“œì™€ í•¨ê»˜ í™•ì¥ ê°€ëŠ¥)\n",
    "\n",
    "    return keyword_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient, ReturnDocument\n",
    "\n",
    "def get_next_sequence(db, name):\n",
    "    counter = db.counters.find_one_and_update(\n",
    "        {\"_id\": name},\n",
    "        {\"$inc\": {\"seq\": 1}},\n",
    "        return_document = ReturnDocument.AFTER,\n",
    "        upsert=True\n",
    "    )\n",
    "    return counter[\"seq\"]\n",
    "\n",
    "\n",
    "def to_save(cur_title, cur_text, cur_images, cur_keyword, category_idx, date_time, reporter, journal, summary):\n",
    "\n",
    "\n",
    "    # MongoDB ì—°ê²° (ê¸°ë³¸ ë¡œì»¬í˜¸ìŠ¤íŠ¸ ì‚¬ìš©, í•„ìš”ì‹œ ì—°ê²° ë¬¸ìì—´ ìˆ˜ì •)\n",
    "    client = MongoClient(\"mongodb+srv://S12P21B201:KqdHQ58L81@ssafy.ngivl.mongodb.net/S12P21B201?authSource=admin\")\n",
    "    db = client[\"S12P21B201\"]           # ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„\n",
    "    collection = db[\"articles\"]      # ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "        \n",
    "\n",
    "    article_id = get_next_sequence(db, \"article_id\")\n",
    "    news_data = {\n",
    "            \"article_id\" : article_id,\n",
    "            \"title\": cur_title,\n",
    "            \"journal\": journal,\n",
    "            \"summary\": summary,\n",
    "            \"reporter\": reporter,\n",
    "            \"datetime\" : date_time,\n",
    "            \"category\": category[category_idx],\n",
    "            \"content\": cur_text,\n",
    "            \"images\": cur_images,\n",
    "            \"quiz_generated\": False,\n",
    "            \"quizzes\": [],\n",
    "            \"correctness\": \"\",\n",
    "            \"keywords\": cur_keyword, \n",
    "        }\n",
    "    json_file_path = \"news_data.json\"\n",
    "    #with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    #    json.dump(news_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # MongoDBì— insert\n",
    "    result = collection.insert_one(news_data)\n",
    "    # print(\"ë‰´ìŠ¤ ë°ì´í„°ê°€ MongoDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. _id:\", result.inserted_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1c2952ebd34dc69a5bd757ea9b37e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a20e9917fe419083f1dcfcf502afde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845679305417483dac407e4b3f76719e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f42f17e2ab42b187d3ef8a9aa59f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ec2a4ffc7945b09bf8339ceed01a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b56a50acf0434583b95eeaeb0a4ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33380f5abdfe4ffebafb3052dcf2f938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0c83c19e0a49dd85782ed16399388c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥í•œ ë°ì´í„° ìˆ˜: 1083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnt = 0\n",
    "for i in tqdm(range(7)):\n",
    "    # print(f\"{i}ë²ˆì§¸ í¬ê¸°: {len(org_link[i])}\")\n",
    "    for j in tqdm(range(len(org_link[i]))):\n",
    "        cur_title, cur_text, cur_images, date_time, reporter, journal, summary = art_crawl(org_link, i, j) # ê¸°ì‚¬ ë°ì´í„° ì €ì¥\n",
    "        if (len(cur_text) > 30 and reporter != \"\"):\n",
    "            cur_keyword = ext_keyword(cur_text)\n",
    "            to_save(cur_title, cur_text, cur_images, cur_keyword, i, date_time, reporter, journal, summary)\n",
    "            cnt += 1\n",
    "\n",
    "print(f\"ì €ì¥í•œ ë°ì´í„° ìˆ˜: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê¸°ì‚¬ ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "ğŸ§  ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n",
      "ğŸ’¾ ìœ ì‚¬ë„ ì €ì¥ ì¤‘...\n",
      "âœ… ìœ ì‚¬ë„ ì €ì¥ ì™„ë£Œ\n",
      "ğŸš€ ëª¨ë“  ìœ ì‚¬ë„ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\n",
      "â³ ì‹¤í–‰ ì‹œê°„: 2.4730ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ğŸ”¹ MongoDB ì—°ê²°\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"techmate\"]\n",
    "\n",
    "# ğŸ”¹ ëª¨ë“  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°\n",
    "def fetch_all_articles():\n",
    "    # articles = list(db.article.find({}, {\"article_id\": 1, \"keywords\": 1}))\n",
    "    articles = list(db.articles.find())\n",
    "    return articles\n",
    "\n",
    "def calculate_similarity(articles):\n",
    "    # 1ï¸âƒ£ ëª¨ë“  ê¸°ì‚¬ì—ì„œ ë“±ì¥í•œ í‚¤ì›Œë“œ ì§‘í•©(ì–´íœ˜ ëª©ë¡) ìƒì„±\n",
    "    vocab = set()\n",
    "    for article in articles:\n",
    "        for kw in article[\"keywords\"]:\n",
    "            vocab.add(kw[\"word\"])\n",
    "    \n",
    "    vocab = sorted(list(vocab))  # ì¼ê´€ì„±ì„ ìœ„í•´ ì •ë ¬\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}  # í‚¤ì›Œë“œ â†’ ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "\n",
    "    # 2ï¸âƒ£ ê¸°ì‚¬ë³„ ê°€ì¤‘ì¹˜ ë²¡í„° ìƒì„± (Sparse Matrix)\n",
    "    article_vectors = np.zeros((len(articles), len(vocab)))\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        for kw in article[\"keywords\"]:\n",
    "            word = kw[\"word\"]\n",
    "            score = kw[\"score\"]  # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
    "            if word in word_to_index:\n",
    "                article_vectors[i, word_to_index[word]] = score  # ê°€ì¤‘ì¹˜ ì ìš©\n",
    "\n",
    "    # 3ï¸âƒ£ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarity_matrix = cosine_similarity(article_vectors)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "# ğŸ”¹ ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥\n",
    "def save_similarity(articles, similarity_matrix, top_n=10, threshold=0.8):\n",
    "    db.similarity.delete_many({})  # ê¸°ì¡´ ìœ ì‚¬ë„ ë°ì´í„° ì‚­ì œ\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        article_id = article[\"article_id\"]\n",
    "        sim_scores = similarity_matrix[i]\n",
    "\n",
    "        # âœ… ìê¸° ìì‹  ì œì™¸í•˜ê³  ìœ ì‚¬ë„ê°€ 0.9 ë¯¸ë§Œì¸ ê¸°ì‚¬ ì¤‘ì—ì„œ ìƒìœ„ Nê°œ ì„ íƒ\n",
    "        similar_articles = [\n",
    "            {\"article_id\": articles[j][\"article_id\"], \"similarity_score\": float(sim_scores[j])}\n",
    "            for j in np.argsort(sim_scores)[::-1] if (i != j and sim_scores[j] < threshold)\n",
    "        ][:top_n]  # ìƒìœ„ Nê°œë§Œ ì €ì¥\n",
    "\n",
    "        # âœ… MongoDBì— ì €ì¥\n",
    "        db.similarity.insert_one({\n",
    "            \"article_id\": article_id,\n",
    "            \"similar_articles\": similar_articles\n",
    "        })\n",
    "\n",
    "    print(\"âœ… ìœ ì‚¬ë„ ì €ì¥ ì™„ë£Œ\")\n",
    "# ğŸ”¹ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_similarity_update():\n",
    "    print(\"ğŸ” ê¸°ì‚¬ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "    articles = fetch_all_articles()\n",
    "    \n",
    "    if len(articles) < 2:\n",
    "        print(\"âŒ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ê¸°ì‚¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ§  ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\")\n",
    "    similarity_matrix = calculate_similarity(articles)\n",
    "\n",
    "    print(\"ğŸ’¾ ìœ ì‚¬ë„ ì €ì¥ ì¤‘...\")\n",
    "    save_similarity(articles, similarity_matrix)\n",
    "\n",
    "    print(\"ğŸš€ ëª¨ë“  ìœ ì‚¬ë„ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.perf_counter()\n",
    "    run_similarity_update()\n",
    "    end_time = time.perf_counter()  # â±ï¸ ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„ ì¸¡ì •\n",
    "    execution_time = end_time - start_time  # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°\n",
    "\n",
    "    print(f\"â³ ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ\")  # ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "# # ğŸ”¹ MongoDB ì—°ê²°\n",
    "# client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "# db = client[\"news_db\"]\n",
    "\n",
    "# # ğŸ”¹ ìƒìœ„ 5ê°œ ì—°ê´€ ê¸°ì‚¬ì˜ íƒ€ì´í‹€, ì¹´í…Œê³ ë¦¬, ë³¸ë¬¸ ì¡°íšŒ (ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì¶”ê°€)\n",
    "# def get_top_related_articles():\n",
    "#     start_time = time.perf_counter()  # â±ï¸ ì‹¤í–‰ ì‹œì‘ ì‹œê°„ ì¸¡ì •\n",
    "\n",
    "#     # 1ï¸âƒ£ ëª¨ë“  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° (title, category, content ì¶”ê°€)\n",
    "#     articles = list(db.articles.find({}, {\"article_id\": 1, \"title\": 1, \"category\": 1, \"content\": 1}))\n",
    "#     article_dict = {article[\"article_id\"]: article for article in articles}  # ì „ì²´ ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "#     # 2ï¸âƒ£ similarity ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬í•œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°\n",
    "#     similarities = db.similarity.find({})\n",
    "\n",
    "#     for sim in similarities:\n",
    "#         article_id = sim[\"article_id\"]\n",
    "#         related_articles = sim[\"similar_articles\"][:5]  # ìƒìœ„ 5ê°œ\n",
    "\n",
    "#         # í˜„ì¬ ê¸°ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "#         current_article = article_dict.get(article_id, {\"title\": \"ê¸°ì‚¬ ì—†ìŒ\", \"category\": \"ì¹´í…Œê³ ë¦¬ ì—†ìŒ\", \"content\": \"ë‚´ìš© ì—†ìŒ\"})\n",
    "#         print(f\"ğŸ“Œ **{current_article['title']}** ({current_article['category']})\")\n",
    "#         # print(f\"   {current_article['content'][:100]}...\")  # ë³¸ë¬¸ ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "#         print(\"\\nğŸ“ **ì—°ê´€ ê¸°ì‚¬ ëª©ë¡:**\")\n",
    "\n",
    "#         # ì—°ê´€ ê¸°ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "#         for i, rel in enumerate(related_articles, 1):\n",
    "#             related_article = article_dict.get(rel[\"article_id\"], {\"title\": \"ê¸°ì‚¬ ì—†ìŒ\", \"category\": \"ì¹´í…Œê³ ë¦¬ ì—†ìŒ\", \"content\": \"ë‚´ìš© ì—†ìŒ\"})\n",
    "#             print(f\"   {i}. [{related_article['category']}] {related_article['title']} \")\n",
    "\n",
    "#         print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "#     end_time = time.perf_counter()  # â±ï¸ ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„ ì¸¡ì •\n",
    "#     execution_time = end_time - start_time  # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°\n",
    "\n",
    "#     print(f\"â³ ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ\")  # ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ ì¶œë ¥\n",
    "\n",
    "# # ì‹¤í–‰\n",
    "# if __name__ == \"__main__\":\n",
    "#     get_top_related_articles()\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ğŸ”¹ MongoDB ì—°ê²°\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"techmate\"]\n",
    "\n",
    "# ğŸ”¹ ìƒìœ„ 5ê°œ ì—°ê´€ ê¸°ì‚¬ì˜ íƒ€ì´í‹€, ì¹´í…Œê³ ë¦¬, ë³¸ë¬¸ ì¡°íšŒ (ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì¶”ê°€)\n",
    "def get_top_related_articles():\n",
    "    start_time = time.perf_counter()  # â±ï¸ ì‹¤í–‰ ì‹œì‘ ì‹œê°„ ì¸¡ì •\n",
    "\n",
    "    # 1ï¸âƒ£ ëª¨ë“  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° (title, category, content ì¶”ê°€)\n",
    "    articles = list(db.articles.find({}, {\"article_id\": 1, \"title\": 1, \"category\": 1, \"content\": 1}))\n",
    "    article_dict = {article[\"article_id\"]: article for article in articles}  # ì „ì²´ ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "    # 2ï¸âƒ£ similarity ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬í•œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°\n",
    "    similarities = db.similarity.find({})\n",
    "\n",
    "    # íŒŒì¼ì— ì¶œë ¥í•  í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    output = []\n",
    "\n",
    "    for sim in similarities:\n",
    "        article_id = sim[\"article_id\"]\n",
    "        related_articles = sim[\"similar_articles\"][:10]  # ìƒìœ„ 5ê°œ\n",
    "\n",
    "        # í˜„ì¬ ê¸°ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        current_article = article_dict.get(article_id, {\"title\": \"ê¸°ì‚¬ ì—†ìŒ\", \"category\": \"ì¹´í…Œê³ ë¦¬ ì—†ìŒ\", \"content\": \"ë‚´ìš© ì—†ìŒ\"})\n",
    "        output.append(f\"ğŸ“Œ **{current_article['title']}** ({current_article['category']})\")\n",
    "        # output.append(f\"   {current_article['content'][:100]}...\")  # ë³¸ë¬¸ ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "        output.append(\"\\nğŸ“ **ì—°ê´€ ê¸°ì‚¬ ëª©ë¡:**\")\n",
    "\n",
    "        # ì—°ê´€ ê¸°ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        for i, rel in enumerate(related_articles, 1):\n",
    "            related_article = article_dict.get(rel[\"article_id\"], {\"title\": \"ê¸°ì‚¬ ì—†ìŒ\", \"category\": \"ì¹´í…Œê³ ë¦¬ ì—†ìŒ\", \"content\": \"ë‚´ìš© ì—†ìŒ\"})\n",
    "            output.append(f\"   {i}. [{related_article['category']}] {related_article['title']} \")\n",
    "\n",
    "        output.append(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    end_time = time.perf_counter()  # â±ï¸ ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„ ì¸¡ì •\n",
    "    execution_time = end_time - start_time  # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°\n",
    "\n",
    "    output.append(f\"â³ ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ\")  # ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ ì¶œë ¥\n",
    "\n",
    "    # output.txt íŒŒì¼ë¡œ ì €ì¥\n",
    "    with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(output))  # ë¦¬ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ í•œ ì¤„ì”© íŒŒì¼ì— ì €ì¥\n",
    "    \n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    get_top_related_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similar_users(target_user_id, user_article_matrix):\n",
    "    # ì‚¬ìš©ì ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarity_matrix = cosine_similarity(user_article_matrix)\n",
    "    similar_users = np.argsort(-similarity_matrix[target_user_id])  # ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    return similar_users[1:3]  # ìê¸° ìì‹  ì œì™¸í•˜ê³  ìƒìœ„ 2ëª… ë°˜í™˜\n",
    "\n",
    "def recommend_from_similar_users(target_user_id, user_article_matrix):\n",
    "    similar_users = get_similar_users(target_user_id, user_article_matrix)\n",
    "    \n",
    "    # ìœ ì‚¬í•œ ì‚¬ìš©ìë“¤ì´ ë§ì´ ë³¸ ê¸°ì‚¬ ì¶”ì²œ\n",
    "    user_scores = np.mean(user_article_matrix[similar_users], axis=0)\n",
    "\n",
    "    # ê¸°ì¡´ì— ì‚¬ìš©ìê°€ ë³¸ ê¸°ì‚¬ ì œì™¸\n",
    "    user_seen = set(np.where(user_article_matrix[target_user_id] > 0)[0])\n",
    "    recommendations = [(i, score) for i, score in enumerate(user_scores) if i not in user_seen]\n",
    "    \n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:5]  # ìƒìœ„ 5ê°œ ì¶”ì²œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_recommendations():\n",
    "    \"\"\"ì‚¬ìš©ìë³„ ì¶”ì²œ ê¸°ì‚¬ ëª©ë¡ ìƒì„±\"\"\"\n",
    "    users = list(users_collection.find({}, {\"_id\": 1, \"search_keywords\": 1, \"interactions\": 1}))\n",
    "    all_articles = list(articles_collection.find({}, {\"_id\": 1, \"keywords\": 1}))\n",
    "\n",
    "    if not users or not all_articles:\n",
    "        return\n",
    "\n",
    "    # ê¸°ì‚¬ ID ë° í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    article_ids = [str(article[\"_id\"]) for article in all_articles]\n",
    "    article_keywords = [\" \".join(article[\"keywords\"]) for article in all_articles]\n",
    "\n",
    "    # TF-IDF ê¸°ë°˜ ë²¡í„°í™”\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    article_tfidf_matrix = vectorizer.fit_transform(article_keywords)\n",
    "\n",
    "    for user in users:\n",
    "        user_id = str(user[\"_id\"])\n",
    "        search_keywords = \" \".join(user.get(\"search_keywords\", []))\n",
    "        interactions = user.get(\"interactions\", [])\n",
    "\n",
    "        # ì‚¬ìš©ì ê²€ìƒ‰ í‚¤ì›Œë“œ ë²¡í„°í™”\n",
    "        user_tfidf_vector = vectorizer.transform([search_keywords])\n",
    "\n",
    "        # ìœ ì‚¬ë„ ê³„ì‚° (ì‚¬ìš©ì ê²€ìƒ‰ í‚¤ì›Œë“œ vs ê¸°ì‚¬ í‚¤ì›Œë“œ)\n",
    "        content_similarities = cosine_similarity(user_tfidf_vector, article_tfidf_matrix).flatten()\n",
    "        top_articles = np.argsort(content_similarities)[::-1][:5]  # ìƒìœ„ 5ê°œ ê¸°ì‚¬ ì„ íƒ\n",
    "\n",
    "        # í˜‘ì—… í•„í„°ë§ - ìœ ì‚¬ ì‚¬ìš©ìê°€ ì¢‹ì•„í•œ ê¸°ì‚¬ ë°˜ì˜\n",
    "        collab_scores = np.zeros(len(all_articles))\n",
    "        for interaction in interactions:\n",
    "            interacted_article_id = interaction[\"article_id\"]\n",
    "            if interacted_article_id in article_ids:\n",
    "                index = article_ids.index(interacted_article_id)\n",
    "                collab_scores += similarity_matrix[index]\n",
    "\n",
    "        # ì½˜í…ì¸  ê¸°ë°˜ + í˜‘ì—… í•„í„°ë§ ì¡°í•©\n",
    "        final_scores = 0.7 * content_similarities + 0.3 * collab_scores\n",
    "        recommended_articles = [article_ids[i] for i in np.argsort(final_scores)[::-1][:5]]\n",
    "\n",
    "        # Redisì— ì €ì¥\n",
    "        redis_client.set(f\"user:{user_id}:recommendations\", \",\".join(recommended_articles))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssafy2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
