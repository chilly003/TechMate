{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ex_tag(sid, date):\n",
    "\n",
    "#     url = f\"https://news.naver.com/breakingnews/section/105/{sid}\"\\\n",
    "#     f\"?date={date}\"\n",
    "#     html = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n",
    "#     \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "#     \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "#     soup = BeautifulSoup(html.text, \"lxml\")\n",
    "#     a_tag = soup.find_all(\"a\", class_=\"sa_text_title _NLOG_IMPRESSION\")\n",
    "    \n",
    "#     tag_lst = []\n",
    "#     print(len(a_tag))\n",
    "#     for a in a_tag:\n",
    "#         imp_url = a.get(\"data-imp-url\") \n",
    "#         if imp_url:  # href가 있는것만 고르는 것\n",
    "#             tag_lst.append(imp_url)  # 리스트에 추가\n",
    "#             print(imp_url)  # 확인용 출력\n",
    "                \n",
    "#     return tag_lst\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def ex_tag(sid, date):\n",
    "    # Selenium 웹 드라이버 설정\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    url = f\"https://news.naver.com/breakingnews/section/105/{sid}?date={date}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    # 페이지 로딩 대기 (최초 로딩 2초 대기)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # \"기사 더보기\" 버튼 클릭 (페이지 하단까지 스크롤)\n",
    "    while True:\n",
    "        try:\n",
    "            # 더보기 버튼 클릭\n",
    "            load_more_button = driver.find_element(By.CSS_SELECTOR, \"a.section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON\")\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # 버튼 클릭 후 잠시 대기\n",
    "        except Exception as e:\n",
    "            # 더보기 버튼이 없으면 (즉, 더 이상 로드할 기사가 없으면) 반복문 종료\n",
    "            print(\"더 이상 기사가 없습니다.\")\n",
    "            break\n",
    "    \n",
    "    # 페이지 소스를 가져와서 BeautifulSoup으로 파싱\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    a_tag = soup.find_all(\"a\", class_=\"sa_text_title _NLOG_IMPRESSION\")\n",
    "    \n",
    "    tag_lst = []\n",
    "    print(len(a_tag))  # 전체 기사 개수 확인\n",
    "    for a in a_tag:\n",
    "        imp_url = a.get(\"data-imp-url\") \n",
    "        if imp_url:  # href가 있는 것만 고르는 것\n",
    "            tag_lst.append(imp_url)  # 리스트에 추가\n",
    "            # print(imp_url)  # 확인용 출력\n",
    "    \n",
    "    # 드라이버 종료\n",
    "    driver.quit()\n",
    "    \n",
    "    return tag_lst\n",
    "\n",
    "# 예시 실행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카테고리별로 html 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "더 이상 기사가 없습니다.\n",
      "64\n",
      "더 이상 기사가 없습니다.\n",
      "105\n",
      "더 이상 기사가 없습니다.\n",
      "122\n",
      "더 이상 기사가 없습니다.\n",
      "709\n",
      "더 이상 기사가 없습니다.\n",
      "21\n",
      "더 이상 기사가 없습니다.\n",
      "66\n",
      "더 이상 기사가 없습니다.\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "category_id = [731, 226, 227, 230, 732, 283, 229] # 모바일, 인터넷/SNS, 통신/뉴미디어, IT 일반, 보안/해킹, 컴퓨터, 게임/리뷰\n",
    "category = [\"모바일\", \"SNS\", \"통신\", \"IT 일반\", \"보안\", \"AI\", \"게임\"]\n",
    "today = \"20250225\"\n",
    "org_link = []\n",
    "\n",
    "for id in category_id:\n",
    "    org_link.append(ex_tag(id, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_tag(sid):\n",
    "    ### 특정 분야의 100페이지까지의 뉴스의 링크를 수집하여 중복 제거한 리스트로 변환하는 함수 ###\n",
    "    re_lst = []\n",
    "    for i in tqdm(range(100)):\n",
    "        lst = ex_tag(sid, i+1)\n",
    "        re_lst.extend(lst)\n",
    "\n",
    "    # 중복 제거\n",
    "    re_set = set(re_lst)\n",
    "    re_lst = list(re_set)\n",
    "    \n",
    "    return re_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원문 / 키워드 추출 후 JSON 형식으로 DB Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def art_crawl(all_hrefs, category_idx, index):\n",
    "\n",
    "    url = all_hrefs[category_idx][index]\n",
    "    html = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 \"\\\n",
    "    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\"\\\n",
    "    \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html.text, \"lxml\")\n",
    "\n",
    "\n",
    "    # -------------------------------------- DATA 추출 --------------------------------------\n",
    "\n",
    "    # 뉴스 제목 추출\n",
    "    title_tag = soup.find(\"meta\", property=\"og:title\")\n",
    "    news_title = \"\"\n",
    "    if title_tag is not None:\n",
    "        news_title = title_tag[\"content\"] if title_tag else soup.find(\"h2\", class_=\"media_end_head_headline\").text.strip()\n",
    "\n",
    "    # 뉴스 본문 추출\n",
    "    article_body = soup.find(\"article\", id=\"dic_area\")\n",
    "    news_content = \"\"\n",
    "    if article_body is not None:\n",
    "        news_content = article_body.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    summary = \"\"\n",
    "    if article_body:\n",
    "    # 요약 부분 추출 (있을 경우)\n",
    "        summary_tag = article_body.find(\"strong\", class_=\"media_end_summary\")\n",
    "        summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "        # 본문 내용에서 요약 부분 제거\n",
    "        if summary_tag:\n",
    "            summary_tag.extract()  # summary_tag를 제거하여 본문에 포함되지 않도록 함\n",
    "\n",
    "        # 나머지 본문 내용 추출 (텍스트만 가져오기)\n",
    "        news_content = article_body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        #print(\"📌 요약:\", summary)\n",
    "        #print(\"\\n📌 본문:\", content)\n",
    "\n",
    "    # 뉴스 이미지 리스트 추출\n",
    "    #image_tags = soup.find_all(\"meta\", property=\"og:image\")\n",
    "\n",
    "    # 여러 개의 이미지 데이터를 저장할 리스트\n",
    "    image_data = []\n",
    "\n",
    "    # 모든 end_photo_org 태그 찾기\n",
    "    photo_tags = soup.find_all(\"span\", class_=\"end_photo_org\")\n",
    "    #print(f\"photo tag 개수: {len(photo_tags)}\")\n",
    "    for photo_tag in photo_tags:\n",
    "        # 이미지 태그 찾기\n",
    "        img_tag = photo_tag.find(\"img\")\n",
    "        #print(img_tag)\n",
    "        img_url = img_tag[\"data-src\"] if img_tag else None  # 이미지가 없는 경우 대비\n",
    "        #print(f\"url: {img_url}\")\n",
    "        # 이미지 설명 태그 찾기 (없을 수도 있음)\n",
    "        desc_tag = photo_tag.find(\"em\", class_=\"img_desc\")\n",
    "        img_desc = desc_tag.get_text(strip=True) if desc_tag else None  # 설명이 없는 경우 None\n",
    "        #print(f\"설명: {img_desc}\")\n",
    "        # 유효한 이미지 데이터만 리스트에 추가\n",
    "        if img_url:\n",
    "            image_data.append({\"image_url\": img_url, \"image_desc\": img_desc})\n",
    "\n",
    "\n",
    "    news_images = []\n",
    "    # img_list.append()\n",
    "    # 뉴스 작성 시간 추출\n",
    "    date_tag = soup.find('span', class_='media_end_head_info_datestamp_time _ARTICLE_DATE_TIME')\n",
    "    date_time = \"\"\n",
    "    if date_tag is not None:\n",
    "        date_time = date_tag['data-date-time']\n",
    "    # 기자 이름 추출\n",
    "    reporter_tag = soup.find('em', class_='media_end_head_journalist_name')\n",
    "    reporter_name = \"\"\n",
    "    if reporter_tag is not None:\n",
    "        reporter_name = reporter_tag.text.strip()\n",
    "        \n",
    "    # 언론사 이름 추출\n",
    "    media_tag = soup.find(\"a\", class_=\"media_end_head_top_logo\")\n",
    "    media_name = media_tag.find(\"img\", class_=\"media_end_head_top_logo_img\")[\"alt\"] if media_tag else \"\"\n",
    "    # print(media_name)\n",
    "    journal = \"\"\n",
    "    # print(\"📌 제목:\", news_title)\n",
    "    # print(\"📌 언론사:\", journal)\n",
    "    # print(\"📌 기자:\", reporter_name)\n",
    "    # print(\"📌 날짜:\", date_time)\n",
    "    # print(\"📌 요약:\", summary)\n",
    "    # print(\"\\n📌 본문:\", news_content)\n",
    "    # print(\"\\n 이미지 개수: \", len(image_data))\n",
    "    if media_name is not None:\n",
    "        journal = media_name     \n",
    "    return news_title, news_content, image_data, date_time, reporter_name, journal, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import konlpy\n",
    "from krwordrank.word import summarize_with_keywords\n",
    "from konlpy.tag import Okt\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "def ext_keyword(cur_text):\n",
    "#     t = Okt()\n",
    "#     content_tokens = t.morphs(cur_text) # 기사 키워드 추출\n",
    "#     print(len(content_tokens))\n",
    "\n",
    "# 키워드 빈도수 포함해서 뽑아내는 방식\n",
    "\n",
    "    min_count = 1   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "    max_length = 10 # 단어의 최대 길이\n",
    "    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "    beta = 0.85    # PageRank의 decaying factor beta\n",
    "    max_iter = 20\n",
    "    #print(\"start\")\n",
    "    keywords, rank, graph = wordrank_extractor.extract([cur_text], beta, max_iter)\n",
    "    #print(len(keywords))\n",
    "\n",
    "    # 키워드를 점수에 따라 내림차순 정렬하고 상위 50개만 선택\n",
    "    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_keywords = sorted_keywords[:50]\n",
    "    \n",
    "    # 각 키워드를 딕셔너리 형태로 변환 (예: {\"word\": \"키워드\", \"score\": 점수})\n",
    "    keyword_list = [{\"word\": word, \"score\": r} for word, r in top_keywords]\n",
    "    \n",
    "    # JSON 형식의 데이터 생성 (필요 시 다른 필드와 함께 확장 가능)\n",
    "\n",
    "    return keyword_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient, ReturnDocument\n",
    "\n",
    "def get_next_sequence(db, name):\n",
    "    counter = db.counters.find_one_and_update(\n",
    "        {\"_id\": name},\n",
    "        {\"$inc\": {\"seq\": 1}},\n",
    "        return_document = ReturnDocument.AFTER,\n",
    "        upsert=True\n",
    "    )\n",
    "    return counter[\"seq\"]\n",
    "\n",
    "\n",
    "def to_save(cur_title, cur_text, cur_images, cur_keyword, category_idx, date_time, reporter, journal, summary):\n",
    "\n",
    "\n",
    "    # MongoDB 연결 (기본 로컬호스트 사용, 필요시 연결 문자열 수정)\n",
    "    client = MongoClient(\"mongodb+srv://S12P21B201:KqdHQ58L81@ssafy.ngivl.mongodb.net/S12P21B201?authSource=admin\")\n",
    "    db = client[\"S12P21B201\"]           # 사용할 데이터베이스 이름\n",
    "    collection = db[\"articles\"]      # 사용할 컬렉션 이름\n",
    "        \n",
    "\n",
    "    article_id = get_next_sequence(db, \"article_id\")\n",
    "    news_data = {\n",
    "            \"article_id\" : article_id,\n",
    "            \"title\": cur_title,\n",
    "            \"journal\": journal,\n",
    "            \"summary\": summary,\n",
    "            \"reporter\": reporter,\n",
    "            \"datetime\" : date_time,\n",
    "            \"category\": category[category_idx],\n",
    "            \"content\": cur_text,\n",
    "            \"images\": cur_images,\n",
    "            \"quiz_generated\": False,\n",
    "            \"quizzes\": [],\n",
    "            \"correctness\": \"\",\n",
    "            \"keywords\": cur_keyword, \n",
    "        }\n",
    "    json_file_path = \"news_data.json\"\n",
    "    #with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    #    json.dump(news_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # MongoDB에 insert\n",
    "    result = collection.insert_one(news_data)\n",
    "    # print(\"뉴스 데이터가 MongoDB에 저장되었습니다. _id:\", result.inserted_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1c2952ebd34dc69a5bd757ea9b37e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a20e9917fe419083f1dcfcf502afde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845679305417483dac407e4b3f76719e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f42f17e2ab42b187d3ef8a9aa59f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ec2a4ffc7945b09bf8339ceed01a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b56a50acf0434583b95eeaeb0a4ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33380f5abdfe4ffebafb3052dcf2f938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0c83c19e0a49dd85782ed16399388c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 데이터 수: 1083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnt = 0\n",
    "for i in tqdm(range(7)):\n",
    "    # print(f\"{i}번째 크기: {len(org_link[i])}\")\n",
    "    for j in tqdm(range(len(org_link[i]))):\n",
    "        cur_title, cur_text, cur_images, date_time, reporter, journal, summary = art_crawl(org_link, i, j) # 기사 데이터 저장\n",
    "        if (len(cur_text) > 30 and reporter != \"\"):\n",
    "            cur_keyword = ext_keyword(cur_text)\n",
    "            to_save(cur_title, cur_text, cur_images, cur_keyword, i, date_time, reporter, journal, summary)\n",
    "            cnt += 1\n",
    "\n",
    "print(f\"저장한 데이터 수: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코사인 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 기사 데이터 로딩 중...\n",
      "🧠 유사도 계산 중...\n",
      "💾 유사도 저장 중...\n",
      "✅ 유사도 저장 완료\n",
      "🚀 모든 유사도 업데이트 완료!\n",
      "⏳ 실행 시간: 2.4730초\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 🔹 MongoDB 연결\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"techmate\"]\n",
    "\n",
    "# 🔹 모든 기사 가져오기\n",
    "def fetch_all_articles():\n",
    "    # articles = list(db.article.find({}, {\"article_id\": 1, \"keywords\": 1}))\n",
    "    articles = list(db.articles.find())\n",
    "    return articles\n",
    "\n",
    "def calculate_similarity(articles):\n",
    "    # 1️⃣ 모든 기사에서 등장한 키워드 집합(어휘 목록) 생성\n",
    "    vocab = set()\n",
    "    for article in articles:\n",
    "        for kw in article[\"keywords\"]:\n",
    "            vocab.add(kw[\"word\"])\n",
    "    \n",
    "    vocab = sorted(list(vocab))  # 일관성을 위해 정렬\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}  # 키워드 → 인덱스 매핑\n",
    "\n",
    "    # 2️⃣ 기사별 가중치 벡터 생성 (Sparse Matrix)\n",
    "    article_vectors = np.zeros((len(articles), len(vocab)))\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        for kw in article[\"keywords\"]:\n",
    "            word = kw[\"word\"]\n",
    "            score = kw[\"score\"]  # 키워드 가중치 사용\n",
    "            if word in word_to_index:\n",
    "                article_vectors[i, word_to_index[word]] = score  # 가중치 적용\n",
    "\n",
    "    # 3️⃣ 코사인 유사도 계산\n",
    "    similarity_matrix = cosine_similarity(article_vectors)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "# 🔹 유사도 결과를 MongoDB에 저장\n",
    "def save_similarity(articles, similarity_matrix, top_n=10, threshold=0.8):\n",
    "    db.similarity.delete_many({})  # 기존 유사도 데이터 삭제\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        article_id = article[\"article_id\"]\n",
    "        sim_scores = similarity_matrix[i]\n",
    "\n",
    "        # ✅ 자기 자신 제외하고 유사도가 0.9 미만인 기사 중에서 상위 N개 선택\n",
    "        similar_articles = [\n",
    "            {\"article_id\": articles[j][\"article_id\"], \"similarity_score\": float(sim_scores[j])}\n",
    "            for j in np.argsort(sim_scores)[::-1] if (i != j and sim_scores[j] < threshold)\n",
    "        ][:top_n]  # 상위 N개만 저장\n",
    "\n",
    "        # ✅ MongoDB에 저장\n",
    "        db.similarity.insert_one({\n",
    "            \"article_id\": article_id,\n",
    "            \"similar_articles\": similar_articles\n",
    "        })\n",
    "\n",
    "    print(\"✅ 유사도 저장 완료\")\n",
    "# 🔹 실행 함수\n",
    "def run_similarity_update():\n",
    "    print(\"🔍 기사 데이터 로딩 중...\")\n",
    "    articles = fetch_all_articles()\n",
    "    \n",
    "    if len(articles) < 2:\n",
    "        print(\"❌ 유사도를 계산할 기사가 부족합니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"🧠 유사도 계산 중...\")\n",
    "    similarity_matrix = calculate_similarity(articles)\n",
    "\n",
    "    print(\"💾 유사도 저장 중...\")\n",
    "    save_similarity(articles, similarity_matrix)\n",
    "\n",
    "    print(\"🚀 모든 유사도 업데이트 완료!\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.perf_counter()\n",
    "    run_similarity_update()\n",
    "    end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "    execution_time = end_time - start_time  # 실행 시간 계산\n",
    "\n",
    "    print(f\"⏳ 실행 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "# # 🔹 MongoDB 연결\n",
    "# client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "# db = client[\"news_db\"]\n",
    "\n",
    "# # 🔹 상위 5개 연관 기사의 타이틀, 카테고리, 본문 조회 (실행 시간 측정 추가)\n",
    "# def get_top_related_articles():\n",
    "#     start_time = time.perf_counter()  # ⏱️ 실행 시작 시간 측정\n",
    "\n",
    "#     # 1️⃣ 모든 기사 가져오기 (title, category, content 추가)\n",
    "#     articles = list(db.articles.find({}, {\"article_id\": 1, \"title\": 1, \"category\": 1, \"content\": 1}))\n",
    "#     article_dict = {article[\"article_id\"]: article for article in articles}  # 전체 기사 딕셔너리\n",
    "\n",
    "#     # 2️⃣ similarity 컬렉션에서 유사한 기사 가져오기\n",
    "#     similarities = db.similarity.find({})\n",
    "\n",
    "#     for sim in similarities:\n",
    "#         article_id = sim[\"article_id\"]\n",
    "#         related_articles = sim[\"similar_articles\"][:5]  # 상위 5개\n",
    "\n",
    "#         # 현재 기사 정보 가져오기\n",
    "#         current_article = article_dict.get(article_id, {\"title\": \"기사 없음\", \"category\": \"카테고리 없음\", \"content\": \"내용 없음\"})\n",
    "#         print(f\"📌 **{current_article['title']}** ({current_article['category']})\")\n",
    "#         # print(f\"   {current_article['content'][:100]}...\")  # 본문 일부만 출력\n",
    "#         print(\"\\n📍 **연관 기사 목록:**\")\n",
    "\n",
    "#         # 연관 기사 정보 가져오기\n",
    "#         for i, rel in enumerate(related_articles, 1):\n",
    "#             related_article = article_dict.get(rel[\"article_id\"], {\"title\": \"기사 없음\", \"category\": \"카테고리 없음\", \"content\": \"내용 없음\"})\n",
    "#             print(f\"   {i}. [{related_article['category']}] {related_article['title']} \")\n",
    "\n",
    "#         print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "#     end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "#     execution_time = end_time - start_time  # 실행 시간 계산\n",
    "\n",
    "#     print(f\"⏳ 실행 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력\n",
    "\n",
    "# # 실행\n",
    "# if __name__ == \"__main__\":\n",
    "#     get_top_related_articles()\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# 🔹 MongoDB 연결\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"techmate\"]\n",
    "\n",
    "# 🔹 상위 5개 연관 기사의 타이틀, 카테고리, 본문 조회 (실행 시간 측정 추가)\n",
    "def get_top_related_articles():\n",
    "    start_time = time.perf_counter()  # ⏱️ 실행 시작 시간 측정\n",
    "\n",
    "    # 1️⃣ 모든 기사 가져오기 (title, category, content 추가)\n",
    "    articles = list(db.articles.find({}, {\"article_id\": 1, \"title\": 1, \"category\": 1, \"content\": 1}))\n",
    "    article_dict = {article[\"article_id\"]: article for article in articles}  # 전체 기사 딕셔너리\n",
    "\n",
    "    # 2️⃣ similarity 컬렉션에서 유사한 기사 가져오기\n",
    "    similarities = db.similarity.find({})\n",
    "\n",
    "    # 파일에 출력할 텍스트를 저장할 리스트\n",
    "    output = []\n",
    "\n",
    "    for sim in similarities:\n",
    "        article_id = sim[\"article_id\"]\n",
    "        related_articles = sim[\"similar_articles\"][:10]  # 상위 5개\n",
    "\n",
    "        # 현재 기사 정보 가져오기\n",
    "        current_article = article_dict.get(article_id, {\"title\": \"기사 없음\", \"category\": \"카테고리 없음\", \"content\": \"내용 없음\"})\n",
    "        output.append(f\"📌 **{current_article['title']}** ({current_article['category']})\")\n",
    "        # output.append(f\"   {current_article['content'][:100]}...\")  # 본문 일부만 출력\n",
    "        output.append(\"\\n📍 **연관 기사 목록:**\")\n",
    "\n",
    "        # 연관 기사 정보 가져오기\n",
    "        for i, rel in enumerate(related_articles, 1):\n",
    "            related_article = article_dict.get(rel[\"article_id\"], {\"title\": \"기사 없음\", \"category\": \"카테고리 없음\", \"content\": \"내용 없음\"})\n",
    "            output.append(f\"   {i}. [{related_article['category']}] {related_article['title']} \")\n",
    "\n",
    "        output.append(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "    execution_time = end_time - start_time  # 실행 시간 계산\n",
    "\n",
    "    output.append(f\"⏳ 실행 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력\n",
    "\n",
    "    # output.txt 파일로 저장\n",
    "    with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(output))  # 리스트의 내용을 한 줄씩 파일에 저장\n",
    "    \n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    get_top_related_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similar_users(target_user_id, user_article_matrix):\n",
    "    # 사용자 간 코사인 유사도 계산\n",
    "    similarity_matrix = cosine_similarity(user_article_matrix)\n",
    "    similar_users = np.argsort(-similarity_matrix[target_user_id])  # 높은 순으로 정렬\n",
    "    return similar_users[1:3]  # 자기 자신 제외하고 상위 2명 반환\n",
    "\n",
    "def recommend_from_similar_users(target_user_id, user_article_matrix):\n",
    "    similar_users = get_similar_users(target_user_id, user_article_matrix)\n",
    "    \n",
    "    # 유사한 사용자들이 많이 본 기사 추천\n",
    "    user_scores = np.mean(user_article_matrix[similar_users], axis=0)\n",
    "\n",
    "    # 기존에 사용자가 본 기사 제외\n",
    "    user_seen = set(np.where(user_article_matrix[target_user_id] > 0)[0])\n",
    "    recommendations = [(i, score) for i, score in enumerate(user_scores) if i not in user_seen]\n",
    "    \n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:5]  # 상위 5개 추천\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_recommendations():\n",
    "    \"\"\"사용자별 추천 기사 목록 생성\"\"\"\n",
    "    users = list(users_collection.find({}, {\"_id\": 1, \"search_keywords\": 1, \"interactions\": 1}))\n",
    "    all_articles = list(articles_collection.find({}, {\"_id\": 1, \"keywords\": 1}))\n",
    "\n",
    "    if not users or not all_articles:\n",
    "        return\n",
    "\n",
    "    # 기사 ID 및 키워드 추출\n",
    "    article_ids = [str(article[\"_id\"]) for article in all_articles]\n",
    "    article_keywords = [\" \".join(article[\"keywords\"]) for article in all_articles]\n",
    "\n",
    "    # TF-IDF 기반 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    article_tfidf_matrix = vectorizer.fit_transform(article_keywords)\n",
    "\n",
    "    for user in users:\n",
    "        user_id = str(user[\"_id\"])\n",
    "        search_keywords = \" \".join(user.get(\"search_keywords\", []))\n",
    "        interactions = user.get(\"interactions\", [])\n",
    "\n",
    "        # 사용자 검색 키워드 벡터화\n",
    "        user_tfidf_vector = vectorizer.transform([search_keywords])\n",
    "\n",
    "        # 유사도 계산 (사용자 검색 키워드 vs 기사 키워드)\n",
    "        content_similarities = cosine_similarity(user_tfidf_vector, article_tfidf_matrix).flatten()\n",
    "        top_articles = np.argsort(content_similarities)[::-1][:5]  # 상위 5개 기사 선택\n",
    "\n",
    "        # 협업 필터링 - 유사 사용자가 좋아한 기사 반영\n",
    "        collab_scores = np.zeros(len(all_articles))\n",
    "        for interaction in interactions:\n",
    "            interacted_article_id = interaction[\"article_id\"]\n",
    "            if interacted_article_id in article_ids:\n",
    "                index = article_ids.index(interacted_article_id)\n",
    "                collab_scores += similarity_matrix[index]\n",
    "\n",
    "        # 콘텐츠 기반 + 협업 필터링 조합\n",
    "        final_scores = 0.7 * content_similarities + 0.3 * collab_scores\n",
    "        recommended_articles = [article_ids[i] for i in np.argsort(final_scores)[::-1][:5]]\n",
    "\n",
    "        # Redis에 저장\n",
    "        redis_client.set(f\"user:{user_id}:recommendations\", \",\".join(recommended_articles))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssafy2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
