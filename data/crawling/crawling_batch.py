import requests
from bs4 import BeautifulSoup
from tqdm.notebook import tqdm
from datetime import datetime, timedelta
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
import konlpy
from krwordrank.word import summarize_with_keywords
from konlpy.tag import Okt
from krwordrank.word import KRWordRank
import pymongo
from pymongo import MongoClient, ReturnDocument
import pickle
from collections import deque
def extract_minutes_ago(article):
    """HTMLì—ì„œ 'në¶„ ì „' ì¶”ì¶œí•˜ì—¬ datetime ë°˜í™˜ (1~10ë¶„ ì‚¬ì´ë§Œ ìœ íš¨)"""
    time_tag = article.find("div", class_="sa_text_datetime is_recent")  
    if not time_tag:
        return None
    
    time_text = time_tag.text.strip()
    now = datetime.utcnow()  # í˜„ì¬ ì‹œê°„ (UTC ê¸°ì¤€)

    # "në¶„ ì „" í˜•íƒœ ì²˜ë¦¬
    minutes_ago_match = re.search(r'(\d+)ë¶„ì „', time_text)
    if minutes_ago_match:
        minutes_ago = int(minutes_ago_match.group(1))
        if 1 <= minutes_ago <= 20:  # 1~10ë¶„ ì „ ê¸°ì‚¬ë§Œ ìœ íš¨
            return now - timedelta(minutes=minutes_ago)
    
    return None  # 10ë¶„ ì´ˆê³¼ ê¸°ì‚¬ëŠ” ì œì™¸


def ex_tag(sid):
    # Selenium ì›¹ ë“œë¼ì´ë²„ ì„¤ì •
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
    url = f"https://news.naver.com/breakingnews/section/105/{sid}"
    driver.get(url)

    soup = BeautifulSoup(driver.page_source, "lxml")
    a_tags = soup.find_all("div", class_="sa_text")  # ê¸°ì‚¬ div ìš”ì†Œ ê°€ì ¸ì˜¤ê¸°
    article_list = []
    for article in a_tags:
        a_tag = article.find("a", class_="sa_text_title _NLOG_IMPRESSION")
        if not a_tag:
            continue

        imp_url = a_tag.get("href")
        article_time = extract_minutes_ago(article)  # ê²Œì‹œ ì‹œê°„ ì¶”ì¶œ

        if imp_url and article_time:
            article_list.append({"url": imp_url, "datetime": article_time})
        # ë“œë¼ì´ë²„ ì¢…ë£Œ
    driver.quit()
    return article_list


def ex_tag_for_day(sid, date):
    # Selenium ì›¹ ë“œë¼ì´ë²„ ì„¤ì •
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
    url = f"https://news.naver.com/breakingnews/section/105/{sid}?date={date}"
    driver.get(url)
    
    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ìµœì´ˆ ë¡œë”© 2ì´ˆ ëŒ€ê¸°)
    time.sleep(2)
    
    # "ê¸°ì‚¬ ë”ë³´ê¸°" ë²„íŠ¼ í´ë¦­ (í˜ì´ì§€ í•˜ë‹¨ê¹Œì§€ ìŠ¤í¬ë¡¤)
    while True:
        try:
            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
            load_more_button = driver.find_element(By.CSS_SELECTOR, "a.section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON")
            load_more_button.click()
            time.sleep(2)  # ë²„íŠ¼ í´ë¦­ í›„ ì ì‹œ ëŒ€ê¸°
        except Exception as e:
            # ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ìœ¼ë©´ (ì¦‰, ë” ì´ìƒ ë¡œë“œí•  ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´) ë°˜ë³µë¬¸ ì¢…ë£Œ
            print("ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            break
    
    # í˜ì´ì§€ ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    soup = BeautifulSoup(driver.page_source, "lxml")
    a_tag = soup.find_all("a", class_="sa_text_title _NLOG_IMPRESSION")
    
    tag_lst = []
    print(len(a_tag))  # ì „ì²´ ê¸°ì‚¬ ê°œìˆ˜ í™•ì¸
    for a in a_tag:
        imp_url = a.get("data-imp-url") 
        if imp_url:  # hrefê°€ ìˆëŠ” ê²ƒë§Œ ê³ ë¥´ëŠ” ê²ƒ
            tag_lst.append({"url": imp_url})  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            # print(imp_url)  # í™•ì¸ìš© ì¶œë ¥
    
    # ë“œë¼ì´ë²„ ì¢…ë£Œ
    driver.quit()
    
    return tag_lst



def art_crawl(all_hrefs, category_idx, index):

    url = all_hrefs[category_idx][index]['url']
    html = requests.get(url, headers = {"User-Agent": "Mozilla/5.0 "\
    "(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)"\
    "Chrome/110.0.0.0 Safari/537.36"})
    soup = BeautifulSoup(html.text, "lxml")
    if html.status_code != 200:
        print(f"IP ì°¨ë‹¨ ê°ì§€: {html.status_code}")


    # -------------------------------------- DATA ì¶”ì¶œ --------------------------------------

    # ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
    title_tag = soup.find("meta", property="og:title")
    news_title = ""
    if title_tag is not None:
        news_title = title_tag["content"] if title_tag else soup.find("h2", class_="media_end_head_headline").text.strip()

    # ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
    article_body = soup.find("article", id="dic_area")
    news_content = ""
    if article_body is not None:
        news_content = article_body.get_text(separator="\n").strip()

    summary = ""
    if article_body:
    # ìš”ì•½ ë¶€ë¶„ ì¶”ì¶œ (ìˆì„ ê²½ìš°)
        summary_tag = article_body.find("strong", class_="media_end_summary")
        summary = summary_tag.get_text(strip=True) if summary_tag else None

        # ë³¸ë¬¸ ë‚´ìš©ì—ì„œ ìš”ì•½ ë¶€ë¶„ ì œê±°
        if summary_tag:
            summary_tag.extract()  # summary_tagë¥¼ ì œê±°í•˜ì—¬ ë³¸ë¬¸ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ í•¨

        # ë‚˜ë¨¸ì§€ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°)
        news_content = article_body.get_text(separator="\n", strip=True)

        #print("ğŸ“Œ ìš”ì•½:", summary)
        #print("\nğŸ“Œ ë³¸ë¬¸:", content)

    # ë‰´ìŠ¤ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    #image_tags = soup.find_all("meta", property="og:image")

    # ì—¬ëŸ¬ ê°œì˜ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    image_data = []

    # ëª¨ë“  end_photo_org íƒœê·¸ ì°¾ê¸°
    photo_tags = soup.find_all("span", class_="end_photo_org")
    #print(f"photo tag ê°œìˆ˜: {len(photo_tags)}")
    for photo_tag in photo_tags:
        # ì´ë¯¸ì§€ íƒœê·¸ ì°¾ê¸°
        img_tag = photo_tag.find("img")
        #print(img_tag)
        img_url = img_tag["data-src"] if img_tag else None  # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„
        #print(f"url: {img_url}")
        # ì´ë¯¸ì§€ ì„¤ëª… íƒœê·¸ ì°¾ê¸° (ì—†ì„ ìˆ˜ë„ ìˆìŒ)
        desc_tag = photo_tag.find("em", class_="img_desc")
        img_desc = desc_tag.get_text(strip=True) if desc_tag else None  # ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš° None
        #print(f"ì„¤ëª…: {img_desc}")
        # ìœ íš¨í•œ ì´ë¯¸ì§€ ë°ì´í„°ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        if img_url:
            image_data.append({"image_url": img_url, "image_desc": img_desc})


    news_images = []
    # img_list.append()
    # ë‰´ìŠ¤ ì‘ì„± ì‹œê°„ ì¶”ì¶œ
    date_tag = soup.find('span', class_='media_end_head_info_datestamp_time _ARTICLE_DATE_TIME')
    date_time = ""
    if date_tag is not None:
        date_time = date_tag['data-date-time']
    # ê¸°ì ì´ë¦„ ì¶”ì¶œ
    reporter_tag = soup.find('em', class_='media_end_head_journalist_name')
    reporter_name = ""
    if reporter_tag is not None:
        reporter_name = reporter_tag.text.strip()
        
    # ì–¸ë¡ ì‚¬ ì´ë¦„ ì¶”ì¶œ
    media_tag = soup.find("a", class_="media_end_head_top_logo")
    media_name = media_tag.find("img", class_="media_end_head_top_logo_img")["alt"] if media_tag else ""
    # print(media_name)
    journal = ""
    # print("ğŸ“Œ ì œëª©:", news_title)
    # print("ğŸ“Œ ì–¸ë¡ ì‚¬:", journal)
    # print("ğŸ“Œ ê¸°ì:", reporter_name)
    # print("ğŸ“Œ ë‚ ì§œ:", date_time)
    # print("ğŸ“Œ ìš”ì•½:", summary)
    # print("\nğŸ“Œ ë³¸ë¬¸:", news_content)
    # print("\n ì´ë¯¸ì§€ ê°œìˆ˜: ", len(image_data))
    if media_name is not None:
        journal = media_name     
    return news_title, news_content, image_data, date_time, reporter_name, journal, summary

category_id = [731, 226, 227, 230, 732, 283, 229] # ëª¨ë°”ì¼, ì¸í„°ë„·/SNS, í†µì‹ /ë‰´ë¯¸ë””ì–´, IT ì¼ë°˜, ë³´ì•ˆ/í•´í‚¹, ì»´í“¨í„°, ê²Œì„/ë¦¬ë·°
category = ["ëª¨ë°”ì¼", "SNS", "í†µì‹ ", "IT ì¼ë°˜", "ë³´ì•ˆ", "AI", "ê²Œì„"]


def ext_keyword(cur_text):
#     t = Okt()
#     content_tokens = t.morphs(cur_text) # ê¸°ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ
#     print(len(content_tokens))

# í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ í¬í•¨í•´ì„œ ë½‘ì•„ë‚´ëŠ” ë°©ì‹

    min_count = 1   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜ (ê·¸ë˜í”„ ìƒì„± ì‹œ)
    max_length = 10 # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´
    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)
    beta = 0.85    # PageRankì˜ decaying factor beta
    max_iter = 20
    #print("start")
    keywords, rank, graph = wordrank_extractor.extract([cur_text], beta, max_iter)
    #print(len(keywords))

    # í‚¤ì›Œë“œë¥¼ ì ìˆ˜ì— ë”°ë¼ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ê³  ìƒìœ„ 50ê°œë§Œ ì„ íƒ
    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
    top_keywords = sorted_keywords[:50]
    
    # ê° í‚¤ì›Œë“œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: {"word": "í‚¤ì›Œë“œ", "score": ì ìˆ˜})
    keyword_list = [{"word": word, "score": r} for word, r in top_keywords]
    
    # JSON í˜•ì‹ì˜ ë°ì´í„° ìƒì„± (í•„ìš” ì‹œ ë‹¤ë¥¸ í•„ë“œì™€ í•¨ê»˜ í™•ì¥ ê°€ëŠ¥)

    return keyword_list


def get_next_sequence(db, name):
    counter = db.counters.find_one_and_update(
        {"_id": name},
        {"$inc": {"seq": 1}},
        return_document = ReturnDocument.AFTER,
        upsert=True
    )
    return counter["seq"]


def to_save(cur_title, cur_text, cur_images, cur_keyword, category_idx, date_time, reporter, journal, summary):


    # MongoDB ì—°ê²° (ê¸°ë³¸ ë¡œì»¬í˜¸ìŠ¤íŠ¸ ì‚¬ìš©, í•„ìš”ì‹œ ì—°ê²° ë¬¸ìì—´ ìˆ˜ì •)
    client = MongoClient("mongodb+srv://S12P21B201:KqdHQ58L81@ssafy.ngivl.mongodb.net/S12P21B201?authSource=admin")
    db = client["S12P21B201"]           # ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ 

    collection = db["articles"]      # ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„
        

    article_id = get_next_sequence(db, "article_id")
    news_data = {
            "article_id" : article_id,
            "title": cur_title,
            "journal": journal,
            "summary": summary,
            "reporter": reporter,
            "datetime" : date_time,
            "category": category[category_idx],
            "content": cur_text,
            "images": cur_images,
            "quiz_generated": False,
            "quizzes": [],
            "correctness": "",
            "keywords": cur_keyword, 
        }
    
    collection.insert_one(news_data)
    return news_data

# MongoDB ì—°ê²° ì„¤ì •
client = MongoClient("mongodb+srv://S12P21B201:KqdHQ58L81@ssafy.ngivl.mongodb.net/S12P21B201?authSource=admin")
db = client["S12P21B201"]     
articles_collection = db["articles"]

# ìµœì´ˆ ì‹¤í–‰: ìµœì‹  1000ê°œ ê°€ì ¸ì™€ì„œ dequeì— ì €ì¥
def initialize_articles():
    articles = list(articles_collection.find()
                    .sort("article_id", -1)
                    .limit(10000))
    dq = deque(articles, maxlen=10000)
    with open("articles_10000.pkl", "wb") as f:
        pickle.dump(dq, f)
    print("ì´ˆê¸° 10000ê°œ ì €ì¥ ì™„ë£Œ")

# ì£¼ê¸°ì  ì—…ë°ì´íŠ¸: 10ë¶„ë§ˆë‹¤ ìƒˆ ë°ì´í„° ì¶”ê°€ & ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
def update_articles(new_articles):
        # articles.pkl ë¶ˆëŸ¬ì˜¤ê¸°
    with open("articles_10000.pkl", "rb") as f:
        dq = pickle.load(f)

    if new_articles:
        print(f"{len(new_articles)}ê°œ ìƒˆ ê¸°ì‚¬ ë°œê²¬, ê°±ì‹  ì¤‘")
        for article in new_articles:
            dq.append(article)  # maxlen=1000 ì´ë¯€ë¡œ ìë™ìœ¼ë¡œ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì‚­ì œë¨
            # ì—…ë°ì´íŠ¸ëœ deque ì €ì¥
        with open("articles_10000.pkl", "wb") as f:
                pickle.dump(dq, f)
    else:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ ì—†ìŒ")

from pymongo import MongoClient
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import time
import pandas as pd
# ğŸ”¹ MongoDB ì—°ê²°
client = MongoClient("mongodb+srv://S12P21B201:KqdHQ58L81@ssafy.ngivl.mongodb.net/S12P21B201?authSource=admin")
db = client["S12P21B201"]           # ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ 

# ğŸ”¹ ëª¨ë“  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
def fetch_all_articles_from_deque():
    # articles.pklì—ì„œ deque ë¶ˆëŸ¬ì˜¤ê¸°
    with open("articles_10000.pkl", "rb") as f:
        dq = pickle.load(f)
    
    # dequeì—ì„œ article ëª©ë¡ ë°˜í™˜
    return list(dq)

def calculate_similarity(articles):
    # 1ï¸âƒ£ ëª¨ë“  ê¸°ì‚¬ì—ì„œ ë“±ì¥í•œ í‚¤ì›Œë“œ ì§‘í•©(ì–´íœ˜ ëª©ë¡) ìƒì„±
    vocab = set()
    for article in articles:
        for kw in article["keywords"]:
            vocab.add(kw["word"])
    
    vocab = sorted(list(vocab))  # ì¼ê´€ì„±ì„ ìœ„í•´ ì •ë ¬
    word_to_index = {word: i for i, word in enumerate(vocab)}  # í‚¤ì›Œë“œ â†’ ì¸ë±ìŠ¤ ë§¤í•‘

    # 2ï¸âƒ£ ê¸°ì‚¬ë³„ ê°€ì¤‘ì¹˜ ë²¡í„° ìƒì„± (Sparse Matrix)
    article_vectors = np.zeros((len(articles), len(vocab)))

    for i, article in enumerate(articles):
        for kw in article["keywords"]:
            word = kw["word"]
            score = kw["score"]  # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì‚¬ìš©
            if word in word_to_index:
                article_vectors[i, word_to_index[word]] = score  # ê°€ì¤‘ì¹˜ ì ìš©

    # 3ï¸âƒ£ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity_matrix = cosine_similarity(article_vectors)
    return similarity_matrix

# ğŸ”¹ ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥
def save_similarity(articles, similarity_matrix, cur_id, cnt, top_n=10, threshold=0.3):
    # `cur_id` ì´í›„ì˜ ê¸°ì‚¬ë§Œ ì²˜ë¦¬

    # ìƒˆë¡­ê²Œ ì¶”ê°€ëœ cntë§Œí¼ì˜ ê¸°ì‚¬ì— ëŒ€í•´ì„œë§Œ ìœ ì‚¬ë„ ì €ì¥
    for i, article in enumerate(articles):
        article_id = article["article_id"]
        #print(article_id, "ë²ˆ ê¸°ì‚¬-----")
        sim_scores = similarity_matrix[i]

        # âœ… ìê¸° ìì‹  ì œì™¸í•˜ê³  ìœ ì‚¬ë„ê°€ 0.9 ë¯¸ë§Œì¸ ê¸°ì‚¬ ì¤‘ì—ì„œ ìƒìœ„ Nê°œ ì„ íƒ
        similar_articles = []
        for j in np.argsort(sim_scores)[::-1]:  # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            if i != j:  # ìê¸° ìì‹  ì œì™¸, threshold ë³´ë‹¤ ìœ ì‚¬ë„ê°€ í° ê²½ìš°ë§Œ
                similar_articles.append({
                    "article_id": articles[j]["article_id"],  # í•´ë‹¹ ê¸°ì‚¬ì—ì„œ article_id ê°€ì ¸ì˜¤ê¸°
                    "similarity_score": float(sim_scores[j])
                })
        
        # ìƒìœ„ Nê°œë§Œ ì €ì¥
        similar_articles = similar_articles[:top_n]

        # âœ… MongoDBì— ì €ì¥
        if article_id > cur_id:
            #print(article_id, "ëŠ” ì €ì¥ì™„ë£Œ")
            db.similarity.update_one(
            {"article_id": article_id},  # í•´ë‹¹ ê¸°ì‚¬ IDê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            {"$set": {"similar_articles": similar_articles}},  # ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
            upsert=True  # ì—†ìœ¼ë©´ ìƒˆë¡œ ì‚½ì…
        )
        

    print("âœ… ìœ ì‚¬ë„ ì €ì¥ ì™„ë£Œ")

import pandas as pd
import numpy as np

def save_similarity_matrix_to_csv(similarity_matrix, file_name="similarity_matrix.csv"):
    # similarity_matrixë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜
    df_similarity = pd.DataFrame(similarity_matrix)
    
    # DataFrameì„ CSVë¡œ ì €ì¥
    df_similarity.to_csv(file_name, index=False, header=False)
    
    print(f"âœ… ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ê°€ '{file_name}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ğŸ”¹ ì‹¤í–‰ í•¨ìˆ˜
def run_similarity_update(cur_id, cnt):

    print("ğŸ” ê¸°ì‚¬ ë°ì´í„° ë¡œë”© ì¤‘...")
    articles = fetch_all_articles_from_deque()
    
    if len(articles) < 2:
        print("âŒ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ê¸°ì‚¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    print("ğŸ§  ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
    similarity_matrix = calculate_similarity(articles)
    
    #save_similarity_matrix_to_csv(similarity_matrix)  # CSVë¡œ ì €ì¥

    print("ğŸ’¾ ìœ ì‚¬ë„ ì €ì¥ ì¤‘...")
    save_similarity(articles, similarity_matrix, cur_id, cnt)

    print("ğŸš€ ëª¨ë“  ìœ ì‚¬ë„ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")




client = MongoClient("mongodb+srv://S12P21B201:KqdHQ58L81@ssafy.ngivl.mongodb.net/S12P21B201?authSource=admin")
db = client["S12P21B201"]           # ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ 

def get_current_sequence(db, name):
    counter = db.counters.find_one({"_id": name})
    return counter["seq"] if counter else None


def main():
    start_time = time.perf_counter()
    org_link = []
    for id in category_id:
        org_link.append(ex_tag(id))
        #org_link.append(ex_tag_for_day(id, "20250411"))
    # ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ

    all_data = []
    cnt = 0

    cur_id = get_current_sequence(db, "article_id")
    print("ì‹œì‘ id: ", cur_id)
    for i in (range(7)):
        for j in (range(len(org_link[i]))):
            cur_title, cur_text, cur_images, date_time, reporter, journal, summary = art_crawl(org_link, i, j)
            if cur_text != "":
                cur_keyword = ext_keyword(cur_text)
                data = to_save(cur_title, cur_text, cur_images, cur_keyword, i, date_time, reporter, journal, summary) # ì—¬ê¸°ì„œ ì´ë¯¸ ë°ì´í„° ê°€ì ¸ì˜¤ë©´ì„œ countersê°€ ì¦ê°€ (article_id ìë™ ì¦ê°€ ìš©ë„)
                all_data.append(data)
                # ë°°ì¹˜ì— ë°ì´í„° ì¶”ê°€
                cnt += 1

    update_articles(all_data) # ìƒˆë¡œ í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ articles_5000ì— ì¶”ê°€í•˜ëŠ” ë¡œì§
    run_similarity_update(cur_id, cnt)# 2000ê°œë¼ë¦¬ë§Œ ìœ ì‚¬ë„ ê³„ì‚° -> ìƒˆë¡œ í¬ë¡¤ë§í•œ ë°ì´í„°ë“¤ë§Œ similarity ì»¬ë ‰ì…˜ì— ìœ ì‚¬ë„ ì €ì¥
    end_time = time.perf_counter()  # â±ï¸ ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„ ì¸¡ì •
    execution_time = end_time - start_time  # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°

    print(f"â³ ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ")  # ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ ì¶œë ¥

print(datetime.now(), " -------------- í¬ë¡¤ë§ ì‹œì‘ --------------")
main()
print(datetime.now(), " -------------- í¬ë¡¤ë§ ì¢…ë£Œ --------------")