{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pymysql\n",
    "import pymongo\n",
    "from datetime import datetime, timedelta\n",
    "#import schedule\n",
    "##import time\n",
    "import time\n",
    "#from bs4 import BeautifulSoup\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pymongo import MongoClient, ReturnDocument\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MySQL 연결 설정\n",
    "# def get_mysql_connection():\n",
    "#     return pymysql.connect(\n",
    "# \t    host='43.200.1.146',\n",
    "#         port=3306,\n",
    "#         user='J12B201',\n",
    "#         password='ssafy12b201',\n",
    "#         db='techmate',\n",
    "#         charset='utf8mb4',\n",
    "#         cursorclass=pymysql.cursors.DictCursor\n",
    "#     )\n",
    "def get_mysql_connection():\n",
    "    return pymysql.connect(\n",
    "\t    host='localhost',\n",
    "        port=3306,\n",
    "        user='root',\n",
    "        password='1234',\n",
    "        db='ssafy',\n",
    "        charset='utf8mb4',\n",
    "        cursorclass=pymysql.cursors.DictCursor\n",
    "    )\n",
    "\n",
    "# MongoDB 연결 설정\n",
    "def get_mongodb_connection():\n",
    "    client = MongoClient(\"mongodb+srv://S12P21B201:KqdHQ58L81@ssafy.ngivl.mongodb.net/S12P21B201?authSource=admin\")\n",
    "    db = client[\"S12P21B201\"]           # 사용할 데이터베이스 이름\n",
    "    return db\n",
    "\n",
    "def fetch_user_interaction_logs():\n",
    "    conn = get_mysql_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            seven_days_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            sql = \"\"\"\n",
    "            SELECT user_id, article_id, \n",
    "                   SUM(CASE WHEN action_type = 'view' THEN 1 ELSE 0 END) as view_count,\n",
    "                   SUM(CASE WHEN action_type = 'like' THEN 3 ELSE 0 END) as like_count,\n",
    "                   SUM(CASE WHEN action_type = 'scrap' THEN 5 ELSE 0 END) as scrap_count,\n",
    "                   SUM(CASE WHEN action_type = 'search' THEN 2 ELSE 0 END) as search_count,\n",
    "                   MAX(created_at) as last_interaction\n",
    "            FROM user_article_interactions\n",
    "            WHERE created_at > %s\n",
    "            GROUP BY user_id, article_id\n",
    "            \"\"\"\n",
    "            cursor.execute(sql, (seven_days_ago,))\n",
    "            results = cursor.fetchall()\n",
    "            df = pd.DataFrame(results)\n",
    "\n",
    "            # ✅ interaction_score 컬럼 생성 (예: view + like + scrap + search 합산)\n",
    "            df['interaction_score'] = (\n",
    "                df['view_count'] +\n",
    "                df['like_count'] +\n",
    "                df['scrap_count'] +\n",
    "                df['search_count']\n",
    "            )\n",
    "\n",
    "            return df\n",
    "    finally:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['ssafy-shard-00-01.ngivl.mongodb.net:27017', 'ssafy-shard-00-02.ngivl.mongodb.net:27017', 'ssafy-shard-00-00.ngivl.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='atlas-2haag7-shard-0', tls=True), 'S12P21B201')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "CANDIDATES_PATH = './models/candidate_articles.pkl'\n",
    "LAST_UPDATE_PATH = './models/last_update.txt'\n",
    "MODEL_PATH = './models/lda_model.pkl'\n",
    "# 뉴스 기사 데이터 가져오기\n",
    "def fetch_articles():\n",
    "    db = get_mongodb_connection()\n",
    "    articles = list(db.articles.find({}, {'_id': 1, 'article_id' : 1, 'title': 1, 'content': 1, 'category': 1, 'datetime': 1}))\n",
    "    print(\"기사 개수 : \")\n",
    "    print(len(articles))\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "\n",
    "# Time Decay 계산 함수\n",
    "def calculate_time_decay(timestamp, decay_factor=0.05):\n",
    "    \"\"\"\n",
    "    Exponential Time Decay 계산 함수\n",
    "    최신 데이터에 더 높은 가중치를 부여\n",
    "    \"\"\"\n",
    "    days_old = (datetime.now() - timestamp).days\n",
    "    return np.exp(-decay_factor * days_old)\n",
    "\n",
    "\n",
    "# 토픽 모델링 기반 후보군 추출 부분\n",
    "def select_candidate_articles(articles):\n",
    "    \"\"\"\n",
    "    토픽 모델링을 활용하여 추천 후보군을 선정하는 함수\n",
    "    \"\"\"\n",
    "    db = get_mongodb_connection()\n",
    "    start_time = time.perf_counter()\n",
    "    # 기사 데이터 가져오기\n",
    "    articles = articles.to_dict(orient='records')\n",
    "    \n",
    "    # 기사 ID와 텍스트 데이터 준비\n",
    "    article_ids = [str(article['article_id']) for article in articles]\n",
    "    texts = [f\"{article['title']} {article['content']}\" for article in articles]\n",
    "    \n",
    "    # 최근 기사에 가중치 부여를 위한 시간 정보\n",
    "    pub_dates = [pd.to_datetime(article.get('datetime', datetime.now())) for article in articles]\n",
    "        \n",
    "    end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "    execution_time = end_time - start_time  # 실행 시간 계산\n",
    "\n",
    "    print(f\"⏳ 데이터 불러오는 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력\n",
    "\n",
    "\n",
    "    # 1. 텍스트 전처리\n",
    "    # 한국어 텍스트인 경우 형태소 분석 등 추가 전처리가 필요할 수 있음\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    start_time = time.perf_counter()\n",
    "    # 불용어 처리 및 벡터화\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95,  # 95% 이상 문서에 등장하는 단어 제외\n",
    "        min_df=2,     # 최소 2개 이상 문서에 등장하는 단어만 포함\n",
    "        max_features=5000,  # 최대 5000개 단어만 사용\n",
    "        stop_words='english'  # 영어 불용어 제거 (한국어의 경우 별도 불용어 리스트 필요)\n",
    "    )\n",
    "    \n",
    "    # 문서-단어 행렬 생성\n",
    "    try:\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "    except ValueError:\n",
    "        logger.error(\"Failed to vectorize article texts. Check if texts are valid.\")\n",
    "        # 실패 시 인기 있는 기사로 대체\n",
    "        return get_popular_articles(300)\n",
    "    \n",
    "    end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "    execution_time = end_time - start_time  # 실행 시간 계산\n",
    "\n",
    "    print(f\"⏳ 단어 행렬 생성 실행 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력\n",
    "    # 2. LDA 토픽 모델링 적용\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # 토픽 수 설정 (이 값은 실험적으로 조정 필요)\n",
    "    n_topics = 20\n",
    "\n",
    "    # LDA 모델 학습\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=10,\n",
    "        learning_method='online',\n",
    "        random_state=42,\n",
    "        batch_size=128,\n",
    "        n_jobs=-1  # 모든 CPU 코어 사용\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 문서-토픽 행렬 생성\n",
    "        doc_topic_matrix = lda.fit_transform(X)\n",
    "        joblib.dump(lda, MODEL_PATH)\n",
    "        print(f\"LDA 모델 저장됨: {MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"LDA model fitting failed: {str(e)}\")\n",
    "        # 실패 시 인기 있는 기사로 대체\n",
    "        return get_popular_articles(300)\n",
    "    \n",
    "    # 3. 각 토픽별 상위 기사 선정\n",
    "    top_articles_per_topic = {}\n",
    "    \n",
    "    # 각 토픽에 대한 기사의 관련도 점수\n",
    "    for topic_idx in range(n_topics):\n",
    "        # 해당 토픽에 대한 각 기사의 점수\n",
    "        topic_scores = doc_topic_matrix[:, topic_idx]\n",
    "        \n",
    "        # 시간 가중치 적용 (최신 기사에 더 높은 가중치)\n",
    "        time_weights = np.array([calculate_time_decay(pub_date) for pub_date in pub_dates])\n",
    "        \n",
    "        # 최종 점수 = 토픽 점수 * 시간 가중치\n",
    "        final_scores = topic_scores * time_weights\n",
    "        \n",
    "        # 점수 기준 상위 기사 선정 (각 토픽별 15개, 전체 300개를 채우기 위함)\n",
    "        top_indices = np.argsort(final_scores)[::-1][:15]\n",
    "        top_articles_per_topic[topic_idx] = [article_ids[idx] for idx in top_indices]\n",
    "    \n",
    "    # 4. 모든 토픽에서 선정된 기사 합치기\n",
    "    candidate_articles = set()\n",
    "    for topic_articles in top_articles_per_topic.values():\n",
    "        candidate_articles.update(topic_articles)\n",
    "    \n",
    "    # 5. 최신 기사 추가 (지난 1일 이내)\n",
    "    recent_articles = [\n",
    "        str(article['_id']) for article in articles\n",
    "        if (datetime.now() - pd.to_datetime(article.get('datetime', datetime.now()))).days <= 1\n",
    "    ]\n",
    "    candidate_articles.update(recent_articles)\n",
    "    \n",
    "    end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "    execution_time = end_time - start_time  # 실행 시간 계산\n",
    "    # 후보군 저장\n",
    "    with open(CANDIDATES_PATH, 'wb') as f:\n",
    "        pickle.dump(list(candidate_articles), f)\n",
    "    print(f\"후보 기사 저장됨: {CANDIDATES_PATH}\")\n",
    "    \n",
    "    # 마지막 업데이트 시간 저장\n",
    "    with open(LAST_UPDATE_PATH, 'w') as f:\n",
    "        f.write(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(f\"⏳ 토픽 모델링링 실행 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력\n",
    "    return candidate_articles\n",
    "\n",
    "# 인기도 기반으로 후보군 선정 (토픽 모델링 실패 시 대체용)\n",
    "def get_popular_articles(n=300):\n",
    "    \"\"\"\n",
    "    사용자 상호작용 기반 인기 기사 선정 (기존 방식)\n",
    "    \"\"\"\n",
    "    user_logs = fetch_user_interaction_logs()\n",
    "    if user_logs.empty:\n",
    "        return set()\n",
    "    \n",
    "    interaction_sum = user_logs['view_count'] + user_logs['like_count'] + user_logs['scrap_count'] + user_logs['search_count']\n",
    "    user_logs['interaction_score'] = interaction_sum\n",
    "    \n",
    "    article_popularity = user_logs.groupby('article_id')['interaction_score'].sum().reset_index()\n",
    "    article_popularity = article_popularity.sort_values('interaction_score', ascending=False)\n",
    "    \n",
    "    popular_articles = set(article_popularity.head(n)['article_id'].tolist())\n",
    "    return popular_articles\n",
    "\n",
    "\n",
    "get_mongodb_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 후보군 불러오기 - 10분 배치에서 사용\n",
    "import os\n",
    "\n",
    "def get_cached_candidate_articles():\n",
    "    \"\"\"\n",
    "    저장된 후보군을 불러오는 함수. 10분 배치에서 사용됨.\n",
    "    만약 후보군이 없거나 오래된 경우(24시간 이상) 인기 있는 기사로 대체\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 마지막 업데이트 시간 확인\n",
    "        if os.path.exists(LAST_UPDATE_PATH):\n",
    "            with open(LAST_UPDATE_PATH, 'r') as f:\n",
    "                last_update_str = f.read().strip()\n",
    "                last_update = datetime.strptime(last_update_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                \n",
    "                # 24시간 이상 지났으면 새로 계산 필요 신호 반환\n",
    "                if datetime.now() - last_update > timedelta(hours=24):\n",
    "                    print(\"후보군이 24시간 이상 지나 만료되었습니다. 인기 기사를 대신 사용합니다.\")\n",
    "                    return get_popular_articles(300)\n",
    "        \n",
    "        # 저장된 후보군 불러오기\n",
    "        if os.path.exists(CANDIDATES_PATH):\n",
    "            with open(CANDIDATES_PATH, 'rb') as f:\n",
    "                candidates = pickle.load(f)\n",
    "                print(f\"저장된 후보 기사 {len(candidates)}개를 불러왔습니다.\")\n",
    "                return candidates\n",
    "        else:\n",
    "            print(\"저장된 후보군이 없습니다. 인기 기사를 대신 사용합니다.\")\n",
    "            return get_popular_articles(300)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load cached candidate articles: {str(e)}\")\n",
    "        return get_popular_articles(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 13:41:29,645 - INFO - Starting recommendation calculation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------log 불러오기 끝-------------------\n",
      "⏳ 로그 추출 실행 시간: 0.2437초\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting recommendation calculation\")\n",
    "    \n",
    "    # 데이터 로드\n",
    "start_time = time.perf_counter()\n",
    "user_logs = fetch_user_interaction_logs()\n",
    "print(\"-------------------log 불러오기 끝-------------------\")\n",
    "end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "execution_time = end_time - start_time  # 실행 시간 계산\n",
    "\n",
    "print(f\"⏳ 로그 추출 실행 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 개수 : \n",
      "23179\n",
      "-------------------기사 불러오기 끝-------------------\n",
      "⏳ 기사 추출 실행 시간: 90.6344초\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.perf_counter()\n",
    "articles_df = fetch_articles()\n",
    "print(\"-------------------기사 불러오기 끝-------------------\")\n",
    "\n",
    "end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "execution_time = end_time - start_time  # 실행 시간 계산\n",
    "\n",
    "print(f\"⏳ 기사 추출 실행 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력\n",
    "    # db = get_mongodb_connection()\n",
    "    \n",
    "    # 유저가 없는 경우 처리\n",
    "if user_logs.empty:\n",
    "    logger.warning(\"No user interaction logs found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index 유무 테스트하기\n",
    "\n",
    "# db.recommendation.find({\"user_id\": \"abc\"}).explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ 데이터 불러오는 시간: 1.3549초\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m----> 2\u001b[0m candidate_articles \u001b[38;5;241m=\u001b[39m \u001b[43mselect_candidate_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()  \u001b[38;5;66;03m# ⏱️ 실행 종료 시간 측정\u001b[39;00m\n\u001b[0;32m      4\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# 실행 시간 계산\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 62\u001b[0m, in \u001b[0;36mselect_candidate_articles\u001b[1;34m(articles)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 문서-단어 행렬 생성\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to vectorize article texts. Check if texts are valid.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\ssafy2\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\ssafy2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\ssafy2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\ssafy2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:109\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\ssafy2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m         )\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_word_ngrams\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "candidate_articles = select_candidate_articles(articles_df)\n",
    "end_time = time.perf_counter()  # ⏱️ 실행 종료 시간 측정\n",
    "execution_time = end_time - start_time  # 실행 시간 계산\n",
    "print(f\"⏳ 후보군 추출 실행 시간: {execution_time:.4f}초\")  # 소수점 4자리까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 후보 기사 23479개를 불러왔습니다.\n",
      "개수 23479\n"
     ]
    }
   ],
   "source": [
    "# print(len(candidate_articles))\n",
    "real_candidate = get_cached_candidate_articles()\n",
    "print(\"개수\", len(real_candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_3796\\1589919642.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  user_article_matrix = user_logs.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "끝\n"
     ]
    }
   ],
   "source": [
    "unique_users = user_logs['user_id'].unique()\n",
    "print(len(unique_users))\n",
    "recommendations = {}\n",
    "db = get_mongodb_connection()\n",
    "user_article_matrix = user_logs.pivot_table(\n",
    "        index='user_id', \n",
    "        columns='article_id', \n",
    "        values='interaction_score',\n",
    "        fill_value=0\n",
    ")   \n",
    "for user_id in unique_users:\n",
    "    user_data = user_logs[user_logs['user_id'] == user_id]    \n",
    "    # 사용자가 이미 상호작용한 기사 목록\n",
    "    user_articles = set(user_data['article_id'].tolist())\n",
    "        \n",
    "        # 사용자 기반 협업 필터링\n",
    "        # 유사 사용자 찾기 (코사인 유사도 사용)\n",
    "        # 사용자-기사 인터랙션 행렬 생성 (가중치 적용)\n",
    "\n",
    "\n",
    "        # 현재 사용자의 벡터\n",
    "    if user_id in user_article_matrix.index:\n",
    "        user_vector = user_article_matrix.loc[user_id].values.reshape(1, -1)\n",
    "        \n",
    "        # 다른 사용자들과의 유사도 계산\n",
    "        similarities = cosine_similarity(user_vector, user_article_matrix.values)[0]\n",
    "            \n",
    "            # 유사도와 사용자 ID를 매핑\n",
    "        sim_users = list(zip(user_article_matrix.index, similarities))\n",
    "        sim_users.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # 상위 30명의 유사 사용자 선택 (자기 자신 제외)\n",
    "        sim_users = [(u, s) for u, s in sim_users if u != user_id][:30]\n",
    "            \n",
    "            # 유사 사용자들이 좋아하는 기사 찾기\n",
    "        cf_scores = defaultdict(float)\n",
    "            \n",
    "        for sim_user, similarity in sim_users:\n",
    "                # 유사 사용자의 상호작용 데이터\n",
    "            sim_user_data = user_logs[user_logs['user_id'] == sim_user]\n",
    "                \n",
    "            for _, row in sim_user_data.iterrows():\n",
    "                article_id = row['article_id']\n",
    "                    \n",
    "                    # 이미 상호작용한 기사 제외 및 후보군으로 제한\n",
    "                if article_id not in user_articles and article_id in candidate_articles:\n",
    "                        # 시간 가중치 적용: 최근 상호작용일수록 더 큰 가중치\n",
    "                    time_weight = calculate_time_decay(row['last_interaction'])\n",
    "                        \n",
    "                        # 협업 필터링 점수 계산\n",
    "                    interaction_score = row['interaction_score']\n",
    "                    cf_scores[article_id] += float(similarity) * float(interaction_score) * float(time_weight)\n",
    "            \n",
    "            # 컨텐츠 기반 필터링 점수 계산\n",
    "        cb_scores = defaultdict(float)\n",
    "            \n",
    "        for article_id in user_articles:\n",
    "                # 사용자가 상호작용한 각 기사와 유사한 기사들 찾기\n",
    "            similar_articles = list(db.article_similarities.find({'article_id': article_id}, {'similar_articles': 1}))\n",
    "                \n",
    "            if similar_articles and 'similar_articles' in similar_articles[0]:\n",
    "                for similar in similar_articles[0]['similar_articles']:\n",
    "                    similar_id = similar['article_id']\n",
    "                        \n",
    "                        # 이미 상호작용한 기사 제외 및 후보군으로 제한\n",
    "                    if similar_id not in user_articles and similar_id in candidate_articles:\n",
    "                        similarity_score = similar['similarity_score']\n",
    "                            \n",
    "                            # 기사 발행 시간에 따른 가중치 계산\n",
    "                        article_info = articles_df[articles_df['_id'] == similar_id]\n",
    "                        if not article_info.empty:\n",
    "                            published_at = article_info.iloc[0]['published_at']\n",
    "                            time_weight = calculate_time_decay(published_at)\n",
    "                                \n",
    "                                # 컨텐츠 기반 점수 계산\n",
    "                            cb_scores[similar_id] += float(similarity_score) * float(time_weight)\n",
    "            \n",
    "            # 두 점수 결합 (가중치 CF:CB = 7:3)\n",
    "        final_scores = {}\n",
    "        for article_id in set(list(cf_scores.keys()) + list(cb_scores.keys())):\n",
    "            final_scores[article_id] = 0.7 * float(cf_scores.get(article_id, 0)) + 0.3 * float(cb_scores.get(article_id, 0))\n",
    "            \n",
    "            # 스코어 기준 상위 8개 기사 선택\n",
    "        top_recommendations = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "            \n",
    "            # 3. 인터리빙 - 탐색(Exploration) 위해 랜덤 기사 2개 추가\n",
    "            # 모든 후보 기사 중에서 아직 추천되지 않은 기사 중 무작위 선택\n",
    "        recommended_ids = set(article_id for article_id, _ in top_recommendations)\n",
    "        exploration_candidates = [\n",
    "            article_id for article_id in candidate_articles \n",
    "            if article_id not in user_articles and article_id not in recommended_ids\n",
    "        ]\n",
    "            \n",
    "        if exploration_candidates:\n",
    "            random_articles = np.random.choice(\n",
    "                exploration_candidates, \n",
    "                size=min(2, len(exploration_candidates)), \n",
    "                replace=False\n",
    "            )\n",
    "            for article_id in random_articles:\n",
    "                top_recommendations.append((article_id, 0))  # 점수는 0으로 설정\n",
    "            \n",
    "            # 최종 추천 목록 저장\n",
    "        recommendations[user_id] = [article_id for article_id, _ in top_recommendations[:10]]\n",
    "    else:\n",
    "        popular_articles = get_popular_articles()\n",
    "            # 새로운 사용자 또는 상호작용이 없는 사용자: 인기 기사 추천\n",
    "        recommendations[user_id] = list(popular_articles)[:10]\n",
    "\n",
    "print(\"끝\")\n",
    "\n",
    "    # MongoDB에 추천 결과 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84, ['11859', '12571', '15896', '11165', '15681', '14023', '14763', '13706', '11754', '16056']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Collection' object is not callable. If you meant to call the 'insert' method on a 'Collection' object it is failing because no such method exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m recommend \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m : user_id,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecommended_articles\u001b[39m\u001b[38;5;124m\"\u001b[39m : recommended_articles\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      7\u001b[0m collection \u001b[38;5;241m=\u001b[39m db[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_recommendations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecommend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# db.user_recommendations.update_one(\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#     {'user_id': user_id},\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     {'$set': {\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\anaconda3\\envs\\ssafy2\\lib\\site-packages\\pymongo\\synchronous\\collection.py:428\u001b[0m, in \u001b[0;36mCollection.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object is not callable. If you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeant to call the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method on a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDatabase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject it is failing because no such method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n\u001b[0;32m    427\u001b[0m     )\n\u001b[1;32m--> 428\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object is not callable. If you meant to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%s\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method on a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object it is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailing because no such method exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    432\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Collection' object is not callable. If you meant to call the 'insert' method on a 'Collection' object it is failing because no such method exists."
     ]
    }
   ],
   "source": [
    "for user_id, recommended_articles in recommendations.items():\n",
    "    print(f\"{user_id}, {recommended_articles}\")\n",
    "    recommend = {\n",
    "        \"user_id\" : user_id,\n",
    "        \"recommended_articles\" : recommended_articles\n",
    "    }\n",
    "    collection = db[\"recommendations\"]\n",
    "    collection.insert_one(recommend)\n",
    "    # db.user_recommendations.update_one(\n",
    "    #     {'user_id': user_id},\n",
    "    #     {'$set': {\n",
    "    #         'recommended_articles': recommended_articles,\n",
    "    #         'updated_at': datetime.now()\n",
    "    #     }\n",
    "    #     }\n",
    "    # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssafy2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
